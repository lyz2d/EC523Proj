{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 150\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m    113\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 114\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images, LAFs[BATCH_SIZE\u001b[38;5;241m*\u001b[39mbatch_idx:BATCH_SIZE\u001b[38;5;241m*\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Add lafs\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    116\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/EC523/EC523Proj/transformer/vit_model.py:185\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x, lafs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lafs):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# If we have LAF as a input, we do not need to extract the LAF from the image\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# lafs = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m#positions = K.feature.laf.get_laf_center(lafs)\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# positions = torch.cat([positions, temp_eig, temp_angle], dim=-1)\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_patch_for_dataset(x,\n\u001b[1;32m    186\u001b[0m                               lafs,\n\u001b[1;32m    187\u001b[0m                               size_resize\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size],\n\u001b[1;32m    188\u001b[0m                               max_point_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_point_num) \u001b[38;5;66;03m# BxPxLxLx3, NHWC format\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# pad it to max_point_num. No need for padding here. Pad in get_patch_for_dataset\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# x = F.pad(x, pad=(0, 0, 0, self.max_point_num - x.shape[1]), mode='constant', value=0) # BxPxLxLx3\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# permute the patches to convert it to NCHW format, BxPxLxLx3 -> BxPx3xLxL \u001b[39;00m\n\u001b[1;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/EC523/EC523Proj/transformer/SIFT/get_patch_and_feature.py:188\u001b[0m, in \u001b[0;36mget_patch_for_dataset\u001b[0;34m(batch_images, LAFs, size_original, size_resize, max_point_num)\u001b[0m\n\u001b[1;32m    185\u001b[0m len_minor\u001b[38;5;241m=\u001b[39mtemp_eig[\u001b[38;5;241m0\u001b[39m,p,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39msize_original\n\u001b[1;32m    186\u001b[0m size\u001b[38;5;241m=\u001b[39msize_resize\n\u001b[0;32m--> 188\u001b[0m temp_resized_patch\u001b[38;5;241m=\u001b[39mget_resized_patch(img,angle,position,len_major,len_minor,size)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# token_from_img.append(temp_resized_patch)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m max_point_num:\n",
      "File \u001b[0;32m~/EC523/EC523Proj/transformer/SIFT/get_patch_and_feature.py:130\u001b[0m, in \u001b[0;36mget_resized_patch\u001b[0;34m(img, angle, position, len_major, len_minor, size, device)\u001b[0m\n\u001b[1;32m    126\u001b[0m new_point \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(rotation_matrix, original_point)\n\u001b[1;32m    127\u001b[0m new_x, new_y \u001b[38;5;241m=\u001b[39m new_point[\u001b[38;5;241m0\u001b[39m,:], new_point[\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m    129\u001b[0m patch\u001b[38;5;241m=\u001b[39mrotated_img[:,:,\n\u001b[0;32m--> 130\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mcat([new_y\u001b[38;5;241m-\u001b[39mlen_minor,torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m]) ]) ) \u001b[38;5;241m.\u001b[39mceil()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint): \n\u001b[1;32m    131\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mmin( torch\u001b[38;5;241m.\u001b[39mcat([new_y\u001b[38;5;241m+\u001b[39mlen_minor,torch\u001b[38;5;241m.\u001b[39mtensor( rotated_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m4\u001b[39m]) ]) )\u001b[38;5;241m.\u001b[39mfloor()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint),\n\u001b[1;32m    132\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mcat([new_x\u001b[38;5;241m-\u001b[39mlen_major,torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m]) ]) )\u001b[38;5;241m.\u001b[39mceil()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint): \n\u001b[1;32m    133\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mmin( torch\u001b[38;5;241m.\u001b[39mcat([new_x\u001b[38;5;241m+\u001b[39mlen_major,torch\u001b[38;5;241m.\u001b[39mtensor( rotated_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]) ]) )\u001b[38;5;241m.\u001b[39mfloor()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint) \n\u001b[1;32m    134\u001b[0m                 ]    \u001b[38;5;66;03m# the first two dimensions of patch match the single input img, which is [1,3,:,:]\u001b[39;00m\n\u001b[1;32m    138\u001b[0m patch\u001b[38;5;241m=\u001b[39mpatch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    139\u001b[0m patch\u001b[38;5;241m=\u001b[39mpatch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# the dimension of patch is [:,:,3]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import kornia.feature as KF\n",
    "\n",
    "from vit_model import ViT  # Import the Vision Transformer model\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model and training parameters\n",
    "IMG_SIZE = 64\n",
    "PATCH_SIZE = 16\n",
    "NUM_CLASSES = 10\n",
    "EMBED_DIM = 768    # Potential problem: too many parameters\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 12\n",
    "MLP_RATIO = 4.0\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_POINT_NUM = 64\n",
    "\n",
    "##########################################################################################\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Data loading and preprocessing\n",
    "# train_transforms = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(64),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])\n",
    "\n",
    "# val_transforms = transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "#     transforms.CenterCrop(64),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# train_dataset = ImageFolder(root='/projectnb/ec523kb/projects/teams_Fall_2024/Team_3/data', transform=train_transforms)\n",
    "# val_dataset = ImageFolder(root='/projectnb/ec523kb/projects/teams_Fall_2024/Team_3/data', transform=val_transforms)\n",
    "\n",
    "# TODO: problem: filter the binary images. Make sure RaFs correspond to the images. \n",
    "\n",
    "# train_dataset = load_dataset(\"zh-plus/tiny-imagenet\", split='train').set_transform(train_transforms)\n",
    "# val_dataset = load_dataset(\"zh-plus/tiny-imagenet\", split='valid').set_transform(val_transforms)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# ds = load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "\n",
    "# train_loader = DataLoader(ds['train'], batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(ds['valid'], batch_size=16, shuffle=False)\n",
    "from SIFT.load_dataset2tensor import load_dataset2tensor\n",
    "\n",
    "train_img, train_label = load_dataset2tensor('Data/train.parquet')\n",
    "train_dataset = TensorDataset(train_img, train_label)\n",
    "\n",
    "# TODO: Add tranforms\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "LAFs = torch.load('Data/LAFs_from_train_set.pt')\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ViT(img_size=IMG_SIZE,\n",
    "            patch_size=PATCH_SIZE,\n",
    "            num_classes=NUM_CLASSES, \n",
    "            embed_dim=EMBED_DIM, \n",
    "            depth=DEPTH, \n",
    "            num_heads=NUM_HEADS, \n",
    "            max_point_num = MAX_POINT_NUM,\n",
    "            mlp_ratio=MLP_RATIO,\n",
    "            feature = KF.KeyNetAffNetHardNet,\n",
    "            )\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training and evaluation functions (from the previous example)\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images, LAFs[BATCH_SIZE*batch_idx:BATCH_SIZE*(batch_idx+1)])  # Add lafs\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Add lafs\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
