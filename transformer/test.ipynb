{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vit_model\n",
    "from vit_model import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = K.io.load_image('Data/image.jpg', K.io.ImageLoadType.RGB32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1216, 0.1059, 0.1059,  ..., 0.1765, 0.2510, 0.2275],\n",
       "         [0.1216, 0.1059, 0.1059,  ..., 0.1412, 0.1804, 0.2627],\n",
       "         [0.1647, 0.1451, 0.1373,  ..., 0.3098, 0.2392, 0.2314],\n",
       "         ...,\n",
       "         [0.3804, 0.3765, 0.4039,  ..., 0.3725, 0.4275, 0.4627],\n",
       "         [0.3882, 0.3569, 0.3647,  ..., 0.3059, 0.3843, 0.4314],\n",
       "         [0.3882, 0.3373, 0.3294,  ..., 0.2549, 0.3216, 0.3608]],\n",
       "\n",
       "        [[0.1333, 0.1176, 0.1176,  ..., 0.2235, 0.2980, 0.2745],\n",
       "         [0.1333, 0.1176, 0.1176,  ..., 0.1843, 0.2235, 0.3059],\n",
       "         [0.1765, 0.1569, 0.1490,  ..., 0.3529, 0.2824, 0.2745],\n",
       "         ...,\n",
       "         [0.4353, 0.4314, 0.4588,  ..., 0.4196, 0.4745, 0.5098],\n",
       "         [0.4471, 0.4157, 0.4235,  ..., 0.3529, 0.4314, 0.4784],\n",
       "         [0.4471, 0.3961, 0.3882,  ..., 0.3020, 0.3686, 0.4078]],\n",
       "\n",
       "        [[0.1686, 0.1529, 0.1529,  ..., 0.1451, 0.2196, 0.1961],\n",
       "         [0.1686, 0.1451, 0.1529,  ..., 0.1176, 0.1569, 0.2392],\n",
       "         [0.2039, 0.1765, 0.1765,  ..., 0.2902, 0.2196, 0.2118],\n",
       "         ...,\n",
       "         [0.3451, 0.3412, 0.3686,  ..., 0.3333, 0.3882, 0.4235],\n",
       "         [0.3569, 0.3255, 0.3333,  ..., 0.2667, 0.3451, 0.3922],\n",
       "         [0.3569, 0.3059, 0.2980,  ..., 0.2157, 0.2824, 0.3216]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = KF.KeyNetAffNetHardNet(5000, True).eval()\n",
    "img1 = K.io.load_image('Data/image.jpg', K.io.ImageLoadType.RGB32)[None, ...]\n",
    "with torch.inference_mode():\n",
    "    lafs1, resps1, descs1 = feature(K.color.rgb_to_grayscale(img1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=vit(img1,lafs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0452, -0.4223,  0.0549, -0.9765,  0.7425,  0.6995, -0.5605, -0.1092,\n",
       "         -0.2246,  0.5895]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(40,10,3,224,224)\n",
    "xs=x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 3, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1,*xs[-3:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[-3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
